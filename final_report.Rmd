---
title: "Bayesian Forecasting of C02 Emissions"
author: "Mallory Wang, Noah Kochanski"
date: "4/13/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
# Library
library(tidyverse)
library(ggplot2)
library(reshape)
library(rstan)
library(rstanarm)
library(glmnet)
library(resample)
library(coda)
library(tidyverse)
library(ggplot2)
library(reshape)
library(corrplot)
library(statip)
library(MASS)
library(tidyverse)
library(leaps)
set.seed(4444)

theme_custom <- function() {
  theme_bw() + # note ggplot2 theme is used as a basis
    theme(plot.title = element_text(size = 10, face = "bold",
                                    hjust = .5,
                                    margin = margin(t = 5, b = 15)),
          plot.caption = element_text(size = 9, hjust = 0, 
                                      margin = margin(t = 15)),
          panel.grid.major = element_line(colour = "grey88"),
          panel.grid.minor = element_blank(),
          legend.title = element_text(size = 10, face = "bold"),
          legend.text = element_text(size = 10),
          axis.text = element_text(size = 10),
          axis.title.x = element_text(margin = margin(t = 10),
                                      size = 10, face = "bold"),
          axis.title.y = element_text(margin = margin(r = 10),
                                      size = 10, face = "bold"))
}

# Data 
us_df = read.csv("us_df.csv")
```


## Introduction

Global warming is the change in the Earth's weather patterns over a extend period of time. Although Earth's climate changing has been a phenomenon that has occurred many times in the past, scientists agree that human activities over the past 100 years are accelerating the speed at which global warming is happening. Carbon dioxide (CO2) is one variable increasing the speed of global warming through the greenhouse gas.  Although CO2 is released through natural processes such as breathing and decomposition, humans have been adding C02 to the atmosphere at unprecedented rates through the burning of fossil fuels. In the United States, there has been an increasing demand to understand, predict, and reduce the carbon footprint as a nation. We look to explore the use of both classical and Bayes linear regression and compare the results when forecasting CO2 emissions in the United States.

## Data Introduction

Our data set of interest comes from the World Bank and includes the following variables tracked from 1960 to present day:

* Year: annual
* C02 Emissions: sourced from Carbon Dioxide Information Analysis Center, Environmental Sciences Division, Oak Ridge National Laboratory, Tennessee, United States measured in kt
* Population: total population sourced from US Census data
* Gross Domestic Product: in current US dollars
* Gross Domestic Income: derived as the sum of GDP and the terms of trade adjustment
* Net Primary Income: in current US dollars
* Population in Urban Agglomeration: population in urban agglomerations of more than one million is the percentage of a country's population living in metropolitan areas that in 2018 had a population of more than one million people
* Energy Use: kg of oil equivalent per capita
* Net Energy Import: % of total energy used
* Electric Power Consumption: kWh per capita

In Figure 1, CO2 emissions in the US is shown to be increasing with time. The dips in the graph typically correspond to times of economic turmoil in the United States. Much of the auxiliary variables included in Figure 1 also depicts increasing functions over time (except GDP growth). Comparing auxiliary variables to one another, we might determine some correlations between them. 

Figure 2 shows a correlation matrix of all variables included in the data. We see many variables have extremely high correlation, as high as 1. As a result, we will want to pick a subset of these variables to avoid singularity when performing matrix calculations. 

```{r, facet, echo = FALSE, warning = FALSE, fig.width=8 ,fig.height=6,fig.cap="\\label{fig:facet} Time series of CO2 and Auxiliary Variables"}
df.m <- melt(us_df, id = c("year"))


ggplot(df.m, aes(year, value)) + 
  geom_line() + 
  facet_wrap(~variable, scales = "free", ncol = 5) +
  theme_custom() +
  scale_x_continuous(breaks=seq(1960, 2020, 40)) 
```

Figure 2 shows a correlation matrix of all variables included in the data. We see many variables have extremely high correlation, as high as 1. As a result, we will want to pick a subset of these variables to avoid singularity when performing matrix calculations. 

In our analysis, we removed population, GDP, GDIncome, and net income due to their high correlation. Additionally, World Bank does not have data for GDI and Income from 1960 to 1970 nor data for Energy Use and Energy Import after 2015. Therefore, the analysis in this report utilizes data from 1970 until 2015. 

```{r correlation plot, echo = FALSE, fig.width=8, fig.height=5, fig.cap="\\label{fig:cor} Correlation matrix of all variables", results = "hide"}
M <- na.omit(us_df[,-1])
colnames(M) <- c("C02", "Population", "GDP", "GDP Growth", "GDI", "Income", "Urban", "Energy", "Energy Import", "Electricity")

corrplot(cor(M),  order = "hclust", type = "lower", mar=c(0,0,2,0), addCoef.col = "black", 
         tl.col="black", tl.srt=20, tl.cex=1, 
         diag=FALSE, 
         number.cex=.5)+ theme_custom()
```

## Methods

In this report, we will first demonstrate the feasibility of a Bayesian linear regression by comparing it to a classical linear regression model, modeling CO2 against only `year` predictor variable. Next, build a Bayesian linear regression model with full auxiliary variables and consider time series related attributes in our data, such as lagging auxiliary variables. A comparison of auxiliary parameters with and without lagged components will be considered. Finally, we conclude with varying windows of posterior parameters estimates with our final Bayesian linear regression model. To reiterate, we use World Bank data depicted in the previous section from 1970 to 2015. Additional segmentation of data will be described in their respective sections. 

### Classical Linear Regression

The first method that we propose to use is by linear regression. Since we are attempting to forecast future CO2 emissions, we use the following model, 
$$Y_t = \beta' \begin{bmatrix} X_{t-5} \\ Y_{t-5} \end{bmatrix} + \epsilon_t$$
where $Y_t$ is the observation at time, $t$, $X_{t-5}, Y_{t-5}$ is the data at time, $t-5$ and $\epsilon_t \sim N(0, \sigma^2)$. To determine the most important variables in predicting CO2 emissions, we perform backward elimination and only keep the 5 most important predictors.

```{r, echo = TRUE}
library(leaps)

# lagging the data 5 years
lag5 <- us_df %>% mutate_all(lag, n = 5) 
colnames(lag5) <- paste(colnames(us_df), "_5", sep="")
lagged_data <- cbind(us_df, lag5) %>% dplyr::select(-c(3:12)) %>% na.omit()

# fitting the full model
full_model <- lm(co2~. ,data = lagged_data)

# backward elimination
models <- regsubsets(co2~., data = lagged_data, nvmax = 5, method = "backward")
summary(models)
```

As seen by the output above, the most important lagged variables which I will include for the following analysis are population, GDI, NPI, population in urban areas, and electric power consumption.  


```{r, echo = FALSE}

# train <- lagged_data %>% na.omit() %>% filter(year %in% 1975:2013) %>% select("co2","pop_5", "gdi_5", "inc_5", "urban_5", "elec_5")
# test <- lagged_data %>% na.omit() %>% filter(year %in% 2014:2018) %>% select("co2", "pop_5", "gdi_5", "inc_5", "urban_5", "elec_5")
# 
# model_1 <- lm(co2~., data = train)
# 
# preds <- data.frame(preds = c(predict(model_1, train[,-1]), predict(model_1, test[,-1])))
# 
# plot_data <- cbind(preds, lagged_data[,1:2])
# 
# 
# ggplot(data = pivot_longer(plot_data, cols = c("preds", "co2")), aes(x= year, y = value, color = name)) + geom_line() + geom_vline(xintercept = 2013) + theme_custom() + ggtitle("United States CO2 Emissions")

```

### Bayes Linear Regression
In this section, we first seek to validate our choice of method by comparing the results of the Bayesian linear regression with the results of the classic linear regression. Next, we consider whether to include lagged variables by comparing their respective Bayes factors in model selection. Finally, we discuss the resulting model and the posterior distribution created.  

First, we start with a basic model where CO2 is explained only by Year. 
$$\begin{aligned}
y_i &= \beta + \beta_{year}x_i + \epsilon_i\\
&= \beta^Tx_i + \epsilon_i
\end{aligned}$$


where $y_i$ is CO2 and $\epsilon_i$ are independently and identically distributed normal with mean zero and constant variance (second line is generalized form for more than one predictor). Under these assumptions, we have the generalized conditional 
$$Y_i|x_i, \beta, \sigma^2 \sim N(\beta^Tx_i + \epsilon_i,\sigma^2)$$
(generalized form will follow from a multivariate normal with mean $X\beta$ and $\sigma^2I$) and the likelihood of $Y_1,...,Y_n$,
$$p(y_1,...,y_n|x_i, \beta, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\bigg(-\frac{(y_i-\beta^Tx_i )^2}{2\sigma^2}\bigg)}$$
  which is maximized when the sum of squared residuals is minimized, which can be written as
$$SSR(\beta) = y^Ty - 2\beta^TX^Ty + \beta^TX^TX\beta$$
  Now we consider the semiconjugated prior of $\beta$,
$$p(y|X,\beta,\sigma^2) \propto \exp{-\frac{1}{2\sigma^2}SSR(\beta)}$$
  and we have the following relationship
$$p(\gamma|y,X,\beta) \sim \text{inverse-gamma}\bigg(\frac{\nu_0+n}{2},\frac{\nu_0\sigma^2_0 + SSR(\beta)}{2}\bigg)$$
  
These specifications (and initial values of 1 and 0.5 for $\nu$ and $\sigma^2$) and were followed to implement a Gibb's sampling to obtain posterior distribution of size 5000. To evaluate these results, we split our data into train and test sets, where 1970 to 2005 were used for train and 2006 to 2015 were used for test. Prediction errors from OLS were compared with the Bayes error (both calculated as sum of squares of test CO2 and CO2 from each method). 

```{r ols, echo = FALSE, fig.width=4,fig.height=2,fig.cap="\\label{fig:ols}Distribution of prediction error difference between OLS and Bayesian methods"}
us_df = read.csv("us_df.csv")
us_df = us_df[which(as.numeric(us_df$year) >= 1970 & as.numeric(us_df$year) <= 2014),]
us_df = us_df[,-which(names(us_df) %in% c("pop","gdp","gdi","inc"))]

S = 5000
X = cbind(rep(1, dim(us_df)[1]), seq(1, dim(us_df)[1], by = 1))
n = dim(X)[1]
p = dim(X)[2]
# Prior
beta0 = c(midrange(us_df$co2), 0)
sigma0 = rbind(c(0.25, 0), c(0, 0.1))
nu0 = 1
s20 = 0.25
inv = solve
# Run linear regression gibbs sampling and obtain a posterior predictive distribution
usco2_pred = apply(t(us_df[,2]), MARGIN = 1, function(y) {
  
  # Store samples
  BETA = matrix(nrow = S, ncol = length(beta0))
  SIGMA = numeric(S)
  
  # Starting values - just use prior values?
  beta = c(midrange(us_df$co2), 0)
  s2 = 0.7^2
  
  # Gibbs sampling algorithm from 9.2.1
  for (s in 1:S) {
    # 1a) Compute V and m
    V = inv(inv(sigma0) + (t(X) %*% X) / s2)
    m = V %*% (inv(sigma0) %*% beta0 + (t(X) %*% y) / s2)
    
    # 1b) sample beta
    beta = mvrnorm(1, m, V)
    
    # 2a) Compute SSR(beta) (specific formula from 9.1)
    ssr = (t(y) %*% y) - (2 * t(beta) %*% t(X) %*% y) + (t(beta) %*% t(X) %*% X %*% beta)
    
    # 2b) sample s2
    s2 = 1 / rgamma(1, (nu0 + n) / 2, (nu0 * s20 + ssr) / 2)
    BETA[s, ] = beta
    SIGMA[s] = s2
  }
  
  # Now sample posterior predictive - two weeks later
  xpred = c(1, 56)
  YPRED = rnorm(S, BETA %*% xpred, sqrt(SIGMA))
  
  YPRED
})
N = 1000
pred_errors = t(sapply(1:N, function(i) {
  y = us_df$co2
  X = as.matrix(us_df[,-2])
  ytrain = y[1:35]
  Xtrain = X[1:35, ]
  ytest = y[-c(1:35)]
  Xtest = X[-c(1:35), ]
  
  # OLS
  beta_ols = inv(t(Xtrain) %*% Xtrain) %*% t(Xtrain) %*% ytrain
  beta_ols
  
  y_ols = Xtest %*% beta_ols
  
  pred_error_ols = sum((ytest - y_ols)^2) / length(ytest)
  
  # Bayes
  y = ytrain
  X = Xtrain
  
  n = dim(X)[1]
  p = dim(X)[2]
  
  g = n
  nu0 = 2
  s20 = 1
  
  S = 1000
  
  Hg = (g / (g + 1)) * X %*% inv(t(X) %*% X) %*% t(X)
  SSRg = t(y) %*% (diag(1, nrow = n) - Hg) %*% y
  
  s2 = 1 / rgamma(S, (nu0 + n) / 2, (nu0 * s20 + SSRg) / 2)
  Vb = g * inv(t(X) %*% X) / (g + 1)
  Eb = Vb %*% t(X) %*% y
  
  E = matrix(rnorm(S * p, 0, sqrt(s2)), S, p)
  beta = t(t(E %*% chol(Vb)) + c(Eb))
  
  beta_bayes = as.matrix(colMeans(beta))
  
  y_bayes = Xtest %*% beta_bayes
  
  pred_error_bayes = sum((ytest - y_bayes)^2) / length(ytest)
  c(pred_error_ols, pred_error_bayes)
})) %>% as.data.frame
colnames(pred_errors) = c('ols', 'bayes')
pred_diff = pred_errors %>% transmute(`bayes - ols` = bayes - ols)
ggplot(pred_diff, aes(x = `bayes - ols`)) +
  geom_density() +
  geom_vline(xintercept = 0, lty = 2)
```

Figure 3 shows the distribution of the difference in prediction error between OLS and Bayesian methods. We can see that Bayes consistently has lower error than OLS, at least when Year is used to predict CO2. With these results, we can add the remaining auxiliary variables; we would use the generalized form of the conditional distribution in above derivations. 

As noted from Figure 1, our data are time series and may rely on time series analysis. In this section, we will evaluate whether lagged variables should be used in the Bayesian linear regression. On a cursory level, we considered whether our outcome variable has some seasonality. Without a full time series analysis, we tried to CO2 data using the classical seasonal decomposition by moving averages and found no seasonality was determined. Next, to replicate some lagged effect, we generated all auxiliary variables, lagged by 5  years and analyzed correlations like before and dropped (in addition to those we already dropped) 5-years lagged population, GDP, GDI, and Income. 

```{r lag, echo = FALSE, fig.width=4,fig.height=2,fig.cap="\\label{fig:lag}Distribution of prediction error difference between OLS and Bayesian methods with lagged variables"}
us_df = read.csv("us_df.csv")
us_df = us_df[which(as.numeric(us_df$year) >= 1970 & as.numeric(us_df$year) <= 2014),]

# Manual Lag
lag5 <- cbind(us_df, us_df[,2:11] %>% mutate_all(lag, n = 5))
colnames(lag5) <- c(colnames(us_df), paste(colnames(us_df[2:11]), "_5", sep=""))

us_df = lag5[,-which(names(lag5) %in% c("pop","gdp","gdi","inc","co2_5","pop_5","gdp_5","gdi_5","inc_5"))]
us_df = us_df[which(as.numeric(us_df$year) >= 1975),]

# Linear Regression
S = 5000
X = cbind(rep(1, dim(us_df)[1]), seq(1, dim(us_df)[1], by = 1))
n = dim(X)[1]
p = dim(X)[2]
# Prior
beta0 = c(midrange(us_df$co2), 0)
sigma0 = rbind(c(0.25, 0), c(0, 0.1))
nu0 = 1
s20 = 0.25
set.seed(4444)
inv = solve
# Run linear regression gibbs sampling and obtain a posterior predictive distribution
usco2_pred = apply(t(us_df[,2]), MARGIN = 1, function(y) {
  
  # Store samples
  BETA = matrix(nrow = S, ncol = length(beta0))
  SIGMA = numeric(S)
  
  # Starting values - just use prior values?
  beta = c(midrange(us_df$co2), 0)
  s2 = 0.7^2
  
  # Gibbs sampling algorithm from 9.2.1
  for (s in 1:S) {
    # 1a) Compute V and m
    V = inv(inv(sigma0) + (t(X) %*% X) / s2)
    m = V %*% (inv(sigma0) %*% beta0 + (t(X) %*% y) / s2)
    
    # 1b) sample beta
    beta = mvrnorm(1, m, V)
    
    # 2a) Compute SSR(beta) (specific formula from 9.1)
    ssr = (t(y) %*% y) - (2 * t(beta) %*% t(X) %*% y) + (t(beta) %*% t(X) %*% X %*% beta)
    
    # 2b) sample s2
    s2 = 1 / rgamma(1, (nu0 + n) / 2, (nu0 * s20 + ssr) / 2)
    BETA[s, ] = beta
    SIGMA[s] = s2
  }
  
  # Now sample posterior predictive - two weeks later
  xpred = c(1, 56)
  YPRED = rnorm(S, BETA %*% xpred, sqrt(SIGMA))
  
  YPRED
})

# Check how good the model is
N = 10000
pred_errors = t(sapply(1:N, function(i) {
  y = us_df$co2
  X = as.matrix(us_df[,-2])
  ytrain = y[1:30]
  Xtrain = X[1:30, ]
  ytest = y[-c(1:30)]
  Xtest = X[-c(1:30), ]
  
  # OLS
  beta_ols = inv(t(Xtrain) %*% Xtrain) %*% t(Xtrain) %*% ytrain
  beta_ols
  
  y_ols = Xtest %*% beta_ols
  
  pred_error_ols = sum((ytest - y_ols)^2) / length(ytest)
  
  # Bayes
  y = ytrain
  X = Xtrain
  
  n = dim(X)[1]
  p = dim(X)[2]
  
  g = n
  nu0 = 2
  s20 = 1
  
  S = 1000
  
  Hg = (g / (g + 1)) * X %*% inv(t(X) %*% X) %*% t(X)
  SSRg = t(y) %*% (diag(1, nrow = n) - Hg) %*% y
  
  s2 = 1 / rgamma(S, (nu0 + n) / 2, (nu0 * s20 + SSRg) / 2)
  Vb = g * inv(t(X) %*% X) / (g + 1)
  Eb = Vb %*% t(X) %*% y
  
  E = matrix(rnorm(S * p, 0, sqrt(s2)), S, p)
  beta = t(t(E %*% chol(Vb)) + c(Eb))
  
  beta_bayes = as.matrix(colMeans(beta))
  
  y_bayes = Xtest %*% beta_bayes
  
  pred_error_bayes = sum((ytest - y_bayes)^2) / length(ytest)
  c(pred_error_ols, pred_error_bayes)
})) %>% as.data.frame
colnames(pred_errors) = c('ols', 'bayes')
pred_diff = pred_errors %>% transmute(`bayes - ols` = bayes - ols)
ggplot(pred_diff, aes(x = `bayes - ols`)) +
  geom_density() +
  geom_vline(xintercept = 0, lty = 2)
```

Figure 4 shows the distribution of the difference in prediction error between OLS and Bayesian methods with lagged variables. Though the distribution with lagged variables still demonstrates improvement over classic linear regression, its center is much higher than the distribution without lagged variables.

We also want to compare the model with and without lagged variable using Bayesian model comparison. Under the believe that many regression coefficients are equal to zero, generate a matrix Z such that $\beta_j = z_j \times b_j$ where $z_j \in \{0,1\}$ and our equation
$$y_i = z_1b_1x_1+...+z_pb_px_p+\epsilon_i$$ 
with $z_j$ indicating which variables has non-zero coefficients. We obtain posterior distribution for z by using the g-prior to evaluate $p(y|X,z)$ for each model z such that we obtain 
$$p(z|y,X) = \frac{p(z)p(y|X,z)}{\sum_z p(z)p(y|X,z)}$$
Using the g-prior distribution for $\beta$, we can compute the marginal probability following
$$\{\beta_z|X_z, \sigma^2\} \sim \text{multivariate noraml}(0,g\sigma^2[X_z^TX_z]^{-1})$$
And we can show the conditional density of $(y,\gamma)$ given $(X,z)$
$$p(y|X,z,\gamma) \times p(\gamma) = (2\pi)^\frac{-n}{2}(1+g)^\frac{-p_z}{2}\times\bigg[\gamma^\frac{n}{2}e^\frac{-\gamma SSR_g^z}{2}\bigg]\times \bigg(\frac{\nu_0\sigma_0^2}{2}\bigg)^\frac{\nu_0}{2}\Gamma\bigg(\frac{\nu_0}{2}\bigg)^{-1}\bigg[\gamma^\frac{\nu_0}{2-1}e^\frac{-\gamma\nu_0\sigma_0^2}{2}\bigg]$$
where 
$$SSR_g^z = y^T\bigg(I = \frac{g}{g+1}X_z(X_z^TX_z)^{-1}X_z\bigg)y$$

Here, we set $g = n$ to use the unit information prior for $p(\sigma^2)$ in modeling z. We follow a similar Gibbs sampling scheme to generate $\{z^{(s+1),\sigma^{(s+1)},\beta^{(s+1)}}\}$ from $z^{(s)}$ by 1) create z matrix 2) in random order sample from $p(z_j|z_{-j},y,X)$ 3) update $z^{(s+1)}$ 3) sample $\sigma^{2(s+1)}$ from $p(\sigma^2|z^{(s+1)},y,X)$ and 5) sample $\beta^{(s+1)}$ from $p(\beta|z^{(s+1)},\sigma^{2(s+1)},y,X)$. 

```{r mslag, echo = FALSE, fig.width=5,fig.height=3,fig.cap="\\label{fig:mslag}Intervals created from beta(s+1) with lagged variables"}
S = 5000
X = as.matrix(us_df[,-2])
y = us_df[,"co2"]
n = dim(X)[1]
p = dim(X)[2]
# Prior
g = n
beta0 = c(midrange(us_df$co2), 0)
sigma0 = rbind(c(0.25, 0), c(0, 0.1))
nu0 = 2
s20 = 1
set.seed(4444)
inv = solve
# Run linear regression with g-prior
Hg <- (g/(g+1)) * X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(y)%*%(diag(1,nrow=n)-Hg)%*%y

s2 <- 1/rgamma(S,(nu0+n)/2, (nu0*s20+SSRg)/2)

Vb <- g*solve(t(X)%*%X)/(g+1)
Eb <- Vb%*%t(X)%*%y

E <- matrix(rnorm(S*p, 0, sqrt(s2)),S,p)
beta <- t(t(E%*%chol(Vb))+c(Eb))

lpy.X <- function(y,X, g=length(y), nu0=1, s20=try(summary(lm(y~1+X))$sigma^2,silent = TRUE)){
  n<-dim(X)[1]
  p<-dim(X)[2]
  if(p==0){Hg <- 0; s20 <- mean(y^2)}
  if(p>0){Hg <- (g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)}
  SSRg <- t(y)%*%(diag(1,nrow=n)-Hg)%*%y
  -0.5*(n*log(pi)+p*log(1+g)+(nu0+n)*log(nu0*s20+SSRg)-nu0*log(nu0*s20))+lgamma((nu0+n)/2)-lgamma(nu0/2)
}
z <- rep(1,dim(X)[2])
lpy.c<-lpy.X(y,X[,z==1,drop=FALSE])
S<-10000
Z<-matrix(NA,S,dim(X)[2])
for(s in 1:S){
  for(j in sample(1:dim(X)[2])){
    zp <- z
    zp[j]<-1-zp[j]
    lpy.p<-lpy.X(y,X[,zp==1,drop=FALSE])
    r<-(lpy.p-lpy.c)*(-1)^(zp[j]==0)
    z[j]<-rbinom(1,1,1/(1+exp(-r)))
    if(z[j]==zp[j]){lpy.c<-lpy.p}
  }
  Z[s,]<-z
}

colSums(Z)/S

X = X*Z[1:dim(X)[1],]
Hg <- (g/(g+1)) * X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(y)%*%(diag(1,nrow=n)-Hg)%*%y

s2 <- 1/rgamma(S,(nu0+n)/2, (nu0*s20+SSRg)/2)

Vb <- g*solve(t(X)%*%X)/(g+1)
Eb <- Vb%*%t(X)%*%y

E <- matrix(rnorm(S*p, 0, sqrt(s2)),S,p)
beta <- t(t(E%*%chol(Vb))+c(Eb))

t(apply(beta, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.5, 0.975)))

signif = apply(beta, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.5, 0.975)) %>%
  apply(MARGIN = 2, FUN = function(y) !(y[1] < 0 && 0 < y[3]))
beta_df = as.data.frame(beta) %>%
  gather(key = 'variable', val = 'coefficient') %>%
  mutate(signif = signif[variable])
ggplot(beta_df, aes(x = variable, y = coefficient, color = signif)) +
  stat_summary(fun = mean, fun.min = function(y) quantile(y, probs = c(0.025)), fun.max = function(y) quantile(y, probs = c(0.975))) +
  geom_hline(yintercept = 0, lty = 2) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

Figure 5 shows intervals created from samples from the $beta^{(s+1)}$ for each predictor grouped by whether they were significant. We can see that most of the 5-years lagged variables are not significant, but 5-year lagged GDP growth is significant with very large interval. This is compared to the same procedure for Gibbs sampling but without the lagged variables. 

```{r ms, cache=TRUE, cache.path="ms/", echo = FALSE, fig.width=5,fig.height=3,fig.cap="\\label{fig:mdlag}Intervals created from beta(s+1)"}
us_df = read.csv("us_df.csv")
us_df = us_df[which(as.numeric(us_df$year) >= 1970 & as.numeric(us_df$year) <= 2014),]
us_df = us_df[,-which(names(us_df) %in% c("pop","gdp","gdi","inc"))]

S = 5000
X = as.matrix(us_df[,-2])
y = us_df[,"co2"]
n = dim(X)[1]
p = dim(X)[2]
# Prior
g = n
beta0 = c(midrange(us_df$co2), 0)
sigma0 = rbind(c(0.25, 0), c(0, 0.1))
nu0 = 2
s20 = 1
set.seed(4444)
inv = solve
# Run linear regression with g-prior
Hg <- (g/(g+1)) * X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(y)%*%(diag(1,nrow=n)-Hg)%*%y

s2 <- 1/rgamma(S,(nu0+n)/2, (nu0*s20+SSRg)/2)

Vb <- g*solve(t(X)%*%X)/(g+1)
Eb <- Vb%*%t(X)%*%y

E <- matrix(rnorm(S*p, 0, sqrt(s2)),S,p)
beta <- t(t(E%*%chol(Vb))+c(Eb))

lpy.X <- function(y,X, g=length(y), nu0=1, s20=try(summary(lm(y~1+X))$sigma^2,silent = TRUE)){
  n<-dim(X)[1]
  p<-dim(X)[2]
  if(p==0){Hg <- 0; s20 <- mean(y^2)}
  if(p>0){Hg <- (g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)}
  SSRg <- t(y)%*%(diag(1,nrow=n)-Hg)%*%y
  -0.5*(n*log(pi)+p*log(1+g)+(nu0+n)*log(nu0*s20+SSRg)-nu0*log(nu0*s20))+lgamma((nu0+n)/2)-lgamma(nu0/2)
}
z <- rep(1,dim(X)[2])
lpy.c<-lpy.X(y,X[,z==1,drop=FALSE])
S<-10000
Z<-matrix(NA,S,dim(X)[2])
for(s in 1:S){
  for(j in sample(1:dim(X)[2])){
    zp <- z
    zp[j]<-1-zp[j]
    lpy.p<-lpy.X(y,X[,zp==1,drop=FALSE])
    r<-(lpy.p-lpy.c)*(-1)^(zp[j]==0)
    z[j]<-rbinom(1,1,1/(1+exp(-r)))
    if(z[j]==zp[j]){lpy.c<-lpy.p}
  }
  Z[s,]<-z
}
X = X*Z[1:dim(X)[1],]
Hg <- (g/(g+1)) * X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(y)%*%(diag(1,nrow=n)-Hg)%*%y

s2 <- 1/rgamma(S,(nu0+n)/2, (nu0*s20+SSRg)/2)

Vb <- g*solve(t(X)%*%X)/(g+1)
Eb <- Vb%*%t(X)%*%y

E <- matrix(rnorm(S*p, 0, sqrt(s2)),S,p)
beta <- t(t(E%*%chol(Vb))+c(Eb))

signif = apply(beta, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.5, 0.975)) %>%
  apply(MARGIN = 2, FUN = function(y) !(y[1] < 0 && 0 < y[3]))
beta_df = as.data.frame(beta) %>%
  gather(key = 'variable', val = 'coefficient') %>%
  mutate(signif = signif[variable])
ggplot(beta_df, aes(x = variable, y = coefficient, color = signif)) +
  stat_summary(fun = mean, fun.min = function(y) quantile(y, probs = c(0.025)), fun.max = function(y) quantile(y, probs = c(0.975))) +
  geom_hline(yintercept = 0, lty = 2) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Figure 6 shows the intervals created from $beta^{(s+1)}$ for each predictor. Unlike Figure 5, all variables are significant. If we return to the previously described test set (data from 2006 to 2015), we can look at how the predicted and observed values compares in Figure 7. The predcited values follow the trend of observed relatively well but the scale is a little off. 

```{r b, echo = FALSE, fig.width=5,fig.height=3,fig.cap="\\label{fig:b}Predicted values vs. observed values in test set"}
y = us_df$co2
X = as.matrix(us_df[,-2])
ytest = y[-c(1:35)]
Xtest = X[-c(1:35),]

beta_bayes = as.matrix(colMeans(beta))
y_bayes = Xtest %*% beta_bayes
bayes_df = data.frame(
  observed = ytest,
  predicted = y_bayes
)
ggplot(bayes_df, aes(x = observed, y = predicted)) +
  geom_point() +
  geom_smooth(method = 'lm')
```

## Conclusion
Our final model has the following posterior distribution for each parameter and can be used to predict future values of CO2. From our analysis, we can conclude that the Bayesian approach improves upon the classic linear regression in this application. 

```{r post, echo = FALSE}
t(apply(beta, MARGIN = 2, FUN = quantile, probs = c(0.025, 0.5, 0.975)))
```

That said, one major limitation of this analysis is its ignorance of time series methodologies and nuances. For example, our linear regression model used only a 5-year lagged variables as predictors. Through the use of moving average (MA) or ARIMA models, we would expect the frequentist approach to improve. Though we did consider basic detection of seasonality and lagged variables, additional consideration of the time series data is probably necessary to make any useful conclusions and to implement in conjunction with a Bayesian approach. Given additional time, more research into time series and Bayesian applications, such as hierarchical regression where some time element could create within group effects, could have contributed significantly to our study, not to mention interesting to learn. 

## References

Hoff, P. D. (2009). A first course in Bayesian statistical methods. New York: Springer.

Gelman, Andrew, Carlin, John B., Stern, Hal S. and Rubin, Donald B.. Bayesian Data Analysis. 2nd ed. : Chapman and Hall/CRC, 2004.



